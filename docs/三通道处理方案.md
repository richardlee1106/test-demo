下面直接回答你的问题：**在“上万 POI、区域分布规律分析”这种场景下，Planner 绝对不能也不应该把所有点丢给 LLM，而是要规划出一套“数据库/算法先算、LLM最后解释”的分层分析方案。**

可以从三个层次来设计 Planner 的行为和整体方案：

---

## 一、Planner 层应该“规划什么”，而不是“看数据本身”

在这个场景里，Planner 的职责是：

> 把用户那句“帮我分析这片区域所有 POI 的分布规律” → 转成**一份结构化分析计划**（Analysis Plan），明确：
> - 要看 **哪个空间范围**（上传的 GeoJSON / 用户画的多边形）
> - 要做 **哪些维度的统计与分析**（业态结构？热点/冷点？与地铁/学校关系？）
> - 每个维度由 **哪个算法/SQL/H3 聚合** 来完成
> - 最后输出给 LLM 的是 **哪几种“压缩后的结果结构”**（汇总指标、网格统计、少量代表点）

Planner 层**不直接接触万级 POI 数据**，只输出类似下面这种 Plan（示意）：

```json
{
  "area": {
    "type": "polygon",
    "source": "user_geojson",
    "value": "{...geojson...}"
  },
  "analysis_goals": [
    "overall_category_distribution",
    "spatial_density_pattern",
    "hotspot_coldspot_detection"
  ],
  "aggregation_strategy": {
    "use_h3": true,
    "h3_resolution": 9,
    "max_grids_for_llm": 80
  },
  "sampling_strategy": {
    "enable": true,
    "cluster_method": "hdbscan",
    "max_representative_pois": 50
  },
  "output_contract_for_llm": {
    "global_stats": true,
    "grid_stats": true,
    "top_hotspots": 5,
    "top_outliers": 10
  }
}
```

也就是说：**Planner 只决定“算什么、怎么算、算到什么粒度供 LLM 看”，真正的重活在 Executor。**

---

## 二、核心方案：用“多层空间聚合 + 代表性抽样”来压缩万级 POI

你的关键诉求是：

- 既要“顾及到选区内所有点”（不能只看少数样本）
- 又要“节省 token”
- 还能“尽可能保证分析概括精度”

这可以通过下面三类技术手段配合完成：

### 1. H3 / Geohash 网格聚合：让 LLM 只看“网格级统计”

**做法：**

1. Planner 发现用户的问题属于“区域整体分布规律”，就选择：
   - `use_h3 = true`
   - 给出一个合适的分辨率（比如 H3 res=8/9，对应几百米级别）
2. Executor 在数据库中执行：
   - 对选区内所有 POI 按 H3 单元分组聚合：
     - 每格：总数量、各大类/小类数量、比例
     - 计算密度指标、集中度指数（如 HHI）、是否为热点/冷点
3. 返回给 LLM 的不是每个 POI，而是一个**有限数量的网格统计列表**，比如 50–100 个六边形，每个只带几十字节信息。

**效果：**

- **覆盖性**：所有 POI 都参与了统计（没有被丢弃），只是被压缩成网格特征。
- **Token 节省**：无论原始 POI 是 1 万还是 10 万，LLM 只看到几十/上百条聚合记录。
- **可解释性**：LLM 可以根据网格密度梯度、类别占比，推断“哪里是核心商圈/生活区/空白区”。

Planner 在这个方案里做的决策包括：

- H3 分辨率（根据区域大小自动选）
- 是否需要多尺度（例如 res=7 + res=9 混用）
- 限制传给 LLM 的网格数量（比如最多 100 个，按代表性排序）

---

### 2. 多层聚合 + “代表点”策略：用少量 POI 讲清结构

有些分析既要看整体，又确实需要举几个**典型例子**，这时可以配合“代表点”策略：

1. **第一层：全量聚合**
   - 同上，用 H3/行政区/自定义小区做聚合，统计整体分布。
2. **第二层：从每类/每热点里挑代表点**
   - 例如：
     - 每个热点 H3 单元选出 1–2 个代表性 POI（中心位置 or 某类占比最高的点）
     - 每种典型业态组合（如“高校周边咖啡+餐饮”）挑若干代表点
   - 总数严格控制在 **几十个以内**（例如 30–50 个）。
3. **第三层：只把代表点 + 聚合统计给 LLM**
   - LLM 在报告里既能说整体结构（基于聚合），又能举出 2–3 个典型例子（基于代表点），用户体验就会非常好。

Planner 在这里要做的是：

- 决定：当前问题是否需要“代表点”（很多“宏观规律”问题其实不需要列具体店名）
- 决定：`max_representative_pois`（如 30 or 50）
- 决定：代表点的选取规则（按密度、按业态多样性、按空间位置均匀性）

---

### 3. 统计特征 + 异常检测：把“模式”而不是“点”丢给 LLM

对于“分布规律”类问题，LLM更关心的是：

- 有没有明显的热点带、中心–边缘结构？
- 哪些业态特别集中/特别缺失？
- 有没有异常点（孤立、过密/过疏）？

这些都可以在 Executor 里通过纯数值计算搞定：

- 空间 Gini 系数（衡量均匀度）
- 密度直方图 / 分位数（判断是否“长尾”）
- 热点检测（Getis-Ord Gi\*, 纯空间统计）
- 简单聚类（DBSCAN/HDBSCAN）找出聚集区、孤立点
- 每类业态的 HHI 指数（集中度）

然后只把**这些指标 + 少量异常点列表**交给 LLM。

Planner 需要做的事情是：

- 根据用户问题，选择需要启用哪些分析模块：
  - 只要宏观 → Gini + 类别占比 + 密度曲线
  - 想看热点 → 再加 Hotspot/Coldspot 模块
  - 想看异常点 → 再加 Outlier 检测模块
- 控制“异常点数量上限”（比如只给前 20 个最极端的）

---

## 三、Planner 层决策逻辑：如何“智能选策略”

你可以把 Planner 的决策逻辑设计成一套简单的规则（也可以以后慢慢用小模型学习），例如：

1. **判断问题类型**
   - 出现“整体分布 / 结构 / 画像 / 特征” → 走“聚合 + 指标”路径
   - 出现“热点 / 冷点 / 集中在哪 / 核心商圈” → 走“热点检测 + 网格聚合”路径
   - 出现“有没有异常 / 特别多 / 明显偏少” → 启用异常检测模块

2. **判断区域规模**
   - 面积小（比如 < 1km²）：
     - 直接用较细 H3 分辨率（res=9/10），网格不多，可以全量给 LLM
   - 面积中等（1–10km²）：
     - 两级 H3（res=8 + res=9）
     - 控制 LLM 输入网格 ≤ 100
   - 面积大（> 10km²）：
     - 只做粗分辨率聚合（res=7/8）+ 少量代表点
     - 不做微观层面的详细展示

3. **判断 POI 数量**
   - `total_pois < 500`：
     - 可以直接全部列给 LLM（如果确实需要）
   - `500–5000`：
     - 网格聚合 + 代表点
   - `> 5000`：
     - 必须强制“只传聚合结果 + 代表点，不传原始点列表”

Planner 的输出里可以明确约束：

```json
{
  "max_raw_pois_for_llm": 0,
  "max_representative_pois": 30,
  "max_grid_count_for_llm": 80
}
```

Executor 和 Writer 都要**严格遵守**这个“配额”。

---

## 四、如何在保证精度的前提下进一步省 token

再补充几个微调建议，让精度与 token 在工程上达到比较“完美”的平衡：

1. **字段裁剪**  
   - 给 LLM 的每个网格/代表点，只保留：
     - 网格：位置标识（H3 id/中心点简写）、总数、主业态、几个关键比例/指数
     - 代表点：name、category、简短类型描述、相对位置（到锚点距离/到某地标方向）
   - 地址经纬度、电话等对“分布规律”没帮助的字段，全部砍掉。

2. **语言压缩**  
   - 对传给 Writer 的 JSON 使用**紧凑字段名**，例如：
     ```json
     {"g":"871fa4c8bffffff","c":123,"m":"餐饮","r":{"餐饮":0.6,"零售":0.2}}
     ```
   - 在 Prompt 里用自然语言解释字段含义，让 LLM 知道 g/c/m/r 分别是啥。

3. **多轮报告而非一次性长文**
   - 第 1 轮：整体画像 + 大致规律
   - 用户如果追问“那热点在哪”“那咖啡分布呢”，再按需加载对应维度的聚合数据。
   - 这样每轮 token 始终可控，问题也更聚焦。

---

## 五、总结一句话方案

> **Planner 的工作不是“把上万个点喂给 LLM”，而是“规划出一套用 H3/聚合/采样/统计把上万个点压缩成几十条高信息密度记录的方案”，再让 LLM 只基于这些“压缩结果”做解释与汇总。**

只要遵循以下硬规则，你的系统既能“顾及到选区内所有点”，又能“Token 极省且分析精度高”：

1. **所有重计算在数据库/算法层完成（Executor），LLM 只看结果**
2. **Planner 必须始终做三件事**：
   - 选空间聚合策略（H3/行政区/聚类）
   - 选统计与异常检测模块（按问题类型）
   - 设定“传给 LLM 的数据配额”（max grids / max representatives）
3. **Writer 只面对“聚合后的 JSON + 少量代表点”，不直接面对原始 POI 列表**

这样设计，你的 Spatial RAG 系统可以**理论上扩展到十万级、百万级 POI**，而 LLM 仍然只消耗几百到一千 token 左右，就足以给出高质量的“区域分布规律”分析。